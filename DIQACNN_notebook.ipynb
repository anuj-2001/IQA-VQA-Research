{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d99552a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\anujp\\dropbox\\my pc (msi)\\downloads\\image-dataset-for-age-prediction-master\\env\\lib\\site-packages (1.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\anujp\\dropbox\\my pc (msi)\\downloads\\image-dataset-for-age-prediction-master\\env\\lib\\site-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: torchvision in c:\\users\\anujp\\dropbox\\my pc (msi)\\downloads\\image-dataset-for-age-prediction-master\\env\\lib\\site-packages (0.10.0)\n",
      "Requirement already satisfied: torch==1.9.0 in c:\\users\\anujp\\dropbox\\my pc (msi)\\downloads\\image-dataset-for-age-prediction-master\\env\\lib\\site-packages (from torchvision) (1.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\anujp\\appdata\\roaming\\python\\python38\\site-packages (from torchvision) (1.19.5)\n",
      "Requirement already satisfied: pillow>=5.3.0 in c:\\users\\anujp\\dropbox\\my pc (msi)\\downloads\\image-dataset-for-age-prediction-master\\env\\lib\\site-packages (from torchvision) (8.2.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\anujp\\dropbox\\my pc (msi)\\downloads\\image-dataset-for-age-prediction-master\\env\\lib\\site-packages (from torch==1.9.0->torchvision) (3.7.4.3)\n",
      "Requirement already satisfied: cv2-extras in c:\\users\\anujp\\dropbox\\my pc (msi)\\downloads\\image-dataset-for-age-prediction-master\\env\\lib\\site-packages (0.5.4)\n",
      "Requirement already satisfied: opencv-python>=4.1 in c:\\users\\anujp\\dropbox\\my pc (msi)\\downloads\\image-dataset-for-age-prediction-master\\env\\lib\\site-packages (from cv2-extras) (4.5.2.54)\n",
      "Requirement already satisfied: matplotlib>=3.0 in c:\\users\\anujp\\dropbox\\my pc (msi)\\downloads\\image-dataset-for-age-prediction-master\\env\\lib\\site-packages (from cv2-extras) (3.3.4)\n",
      "Requirement already satisfied: numpy>=1.16 in c:\\users\\anujp\\appdata\\roaming\\python\\python38\\site-packages (from cv2-extras) (1.19.5)\n",
      "Requirement already satisfied: scikit-image>=0.15 in c:\\users\\anujp\\appdata\\roaming\\python\\python38\\site-packages (from cv2-extras) (0.18.1)\n",
      "Requirement already satisfied: scipy>=1.2 in c:\\users\\anujp\\dropbox\\my pc (msi)\\downloads\\image-dataset-for-age-prediction-master\\env\\lib\\site-packages (from cv2-extras) (1.4.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\anujp\\dropbox\\my pc (msi)\\downloads\\image-dataset-for-age-prediction-master\\env\\lib\\site-packages (from matplotlib>=3.0->cv2-extras) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\anujp\\dropbox\\my pc (msi)\\downloads\\image-dataset-for-age-prediction-master\\env\\lib\\site-packages (from matplotlib>=3.0->cv2-extras) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\anujp\\dropbox\\my pc (msi)\\downloads\\image-dataset-for-age-prediction-master\\env\\lib\\site-packages (from matplotlib>=3.0->cv2-extras) (8.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\anujp\\dropbox\\my pc (msi)\\downloads\\image-dataset-for-age-prediction-master\\env\\lib\\site-packages (from matplotlib>=3.0->cv2-extras) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\anujp\\dropbox\\my pc (msi)\\downloads\\image-dataset-for-age-prediction-master\\env\\lib\\site-packages (from matplotlib>=3.0->cv2-extras) (2.8.1)\n",
      "Requirement already satisfied: six in c:\\users\\anujp\\appdata\\roaming\\python\\python38\\site-packages (from cycler>=0.10->matplotlib>=3.0->cv2-extras) (1.15.0)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\users\\anujp\\appdata\\roaming\\python\\python38\\site-packages (from scikit-image>=0.15->cv2-extras) (1.1.1)\n",
      "Requirement already satisfied: networkx>=2.0 in c:\\users\\anujp\\dropbox\\my pc (msi)\\downloads\\image-dataset-for-age-prediction-master\\env\\lib\\site-packages (from scikit-image>=0.15->cv2-extras) (2.5.1)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in c:\\users\\anujp\\appdata\\roaming\\python\\python38\\site-packages (from scikit-image>=0.15->cv2-extras) (2021.6.14)\n",
      "Requirement already satisfied: imageio>=2.3.0 in c:\\users\\anujp\\appdata\\roaming\\python\\python38\\site-packages (from scikit-image>=0.15->cv2-extras) (2.9.0)\n",
      "Requirement already satisfied: decorator<5,>=4.3 in c:\\users\\anujp\\dropbox\\my pc (msi)\\downloads\\image-dataset-for-age-prediction-master\\env\\lib\\site-packages (from networkx>=2.0->scikit-image>=0.15->cv2-extras) (4.4.2)\n",
      "Requirement already satisfied: xlrd in c:\\users\\anujp\\dropbox\\my pc (msi)\\downloads\\image-dataset-for-age-prediction-master\\env\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\anujp\\dropbox\\my pc (msi)\\downloads\\image-dataset-for-age-prediction-master\\env\\lib\\site-packages (3.0.7)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\anujp\\dropbox\\my pc (msi)\\downloads\\image-dataset-for-age-prediction-master\\env\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: pytorch-ignite in c:\\users\\anujp\\dropbox\\my pc (msi)\\downloads\\image-dataset-for-age-prediction-master\\env\\lib\\site-packages (0.4.5)\n",
      "Requirement already satisfied: torch<2,>=1.3 in c:\\users\\anujp\\dropbox\\my pc (msi)\\downloads\\image-dataset-for-age-prediction-master\\env\\lib\\site-packages (from pytorch-ignite) (1.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\anujp\\dropbox\\my pc (msi)\\downloads\\image-dataset-for-age-prediction-master\\env\\lib\\site-packages (from torch<2,>=1.3->pytorch-ignite) (3.7.4.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install torchvision\n",
    "!pip install cv2-extras\n",
    "!pip install xlrd\n",
    "!pip install openpyxl\n",
    "!pip install pytorch-ignite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d703323f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf5df7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-c247007c1d20>:23: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  config=yaml.load(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img3.bmp\n"
     ]
    }
   ],
   "source": [
    "class DataInfoLoader:\n",
    "    def __init__(self,dataset_name,config):\n",
    "        self.config=config\n",
    "        self.dataset_name=dataset_name\n",
    "        self.img_num=len(pd.read_excel(config[dataset_name]['gt_file_path'])['img_name'])\n",
    "        self.IQA_results_path=config[dataset_name]['IQA_results_path']\n",
    "    \n",
    "    def get_qs_std(self):\n",
    "        return pd.read_excel(self.config[self.dataset_name]['gt_file_path'])['acc_avg']\n",
    "\n",
    "    def get_img_name(self):\n",
    "        return pd.read_excel(self.config[self.dataset_name]['gt_file_path'])['img_name']\n",
    "    \n",
    "    def get_img_set(self):\n",
    "        return pd.read_excel(self.config[self.dataset_name]['gt_file_path'])['img_set']\n",
    "\n",
    "    def get_img_path(self):\n",
    "        \"Get the image path for all images\"\n",
    "        return [self.config[self.dataset_name]['root']+'/'+name for name in self.get_img_name()]\n",
    "\n",
    "if __name__=='__main__':\n",
    "    with open('./config.yaml') as f:\n",
    "        config=yaml.load(f)\n",
    "    dataset_name='SOC'\n",
    "    dil=DataInfoLoader(dataset_name,config)\n",
    "    img_name=dil.get_img_name()\n",
    "    print(img_name[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d9ba983",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "from PIL import Image\n",
    "from scipy.signal import convolve2d\n",
    "import numpy as np\n",
    "import yaml\n",
    "import math\n",
    "from pathlib import Path\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7560623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_loader(path):\n",
    "    return Image.open(path).convert('L')\n",
    "\n",
    "def localNormalization(patch, P=3, Q=3, C=1):\n",
    "    kernel = np.ones((P,Q)) / (P * Q)\n",
    "    patch_mean = convolve2d(patch, kernel, boundary='symm', mode='same')\n",
    "    patch_sm = convolve2d(np.square(patch), kernel, boundary='symm', mode='same')\n",
    "    patch_std = np.sqrt(np.maximum(patch_sm - np.square(patch_mean), 0)) + C\n",
    "    patch_ln = (patch - patch_mean)/patch_std\n",
    "    return patch_ln\n",
    "\n",
    "def patchSifting(im, patch_size=48, stride=48):\n",
    "    img=np.array(im).copy()\n",
    "    im1=localNormalization(img)\n",
    "    im1=Image.fromarray(im1)\n",
    "    ret1,im2= cv2.threshold(img,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    w, h = im1.size\n",
    "    patches=()\n",
    "    for i in range(0, h - stride, stride):\n",
    "        for j in range(0, w - stride, stride):\n",
    "            patch = im2[i:i+patch_size,j:j+patch_size]\n",
    "            if judgeAllOnesOrAllZreos(patch)==False:\n",
    "                patch=to_tensor(im1.crop((j,i,j+patch_size,i+patch_size)))\n",
    "                patch=patch.float().unsqueeze(0)\n",
    "                patches = patches + (patch,)\n",
    "    return patches\n",
    "def judgeAllOnesOrAllZreos(patch):\n",
    "    flag1=True\n",
    "    flag2=True\n",
    "    for i in range(patch.shape[0]):\n",
    "        for j in range(patch.shape[1]):\n",
    "            if patch[i,j]==255:\n",
    "                continue\n",
    "            else:\n",
    "                flag1=False\n",
    "\n",
    "    for i in range(patch.shape[0]):\n",
    "        for j in range(patch.shape[1]):\n",
    "            if patch[i,j]==0:\n",
    "                continue\n",
    "            else:\n",
    "                flag2=False\n",
    "    return flag1 or flag2\n",
    "class DIQADataset(Dataset):\n",
    "    def __init__(self,dataset_name,config,data_index,status='train',loader=default_loader):\n",
    "        self.loader = loader\n",
    "        self.patch_size = 48\n",
    "        self.stride = 48\n",
    "        test_ratio = config['test_ratio']  \n",
    "        train_ratio = config['train_ratio']\n",
    "        dil=DataInfoLoader(dataset_name,config)\n",
    "        img_name=dil.get_img_name()\n",
    "        img_path=dil.get_img_path()\n",
    "        qs_std=dil.get_qs_std()\n",
    "\n",
    "        # print(f\"{img_name} {img_path} {qs_std}\")\n",
    "\n",
    "        if status == 'train':\n",
    "            self.index = data_index\n",
    "            print(\"# Train Images: {}\".format(len(self.index)))\n",
    "        if status == 'test':\n",
    "            self.index = data_index\n",
    "            print(\"# Test Images: {}\".format(len(self.index)))\n",
    "        if status == 'val':\n",
    "            self.index = data_index\n",
    "            print(\"# Val Images: {}\".format(len(self.index)))\n",
    "        print('Index:')\n",
    "        print(self.index)\n",
    "\n",
    "        self.patches = ()\n",
    "        self.label = []\n",
    "        ps=1\n",
    "        for idx in self.index:\n",
    "            im=self.loader(img_path[idx])\n",
    "            patches = patchSifting(im)\n",
    "            print(f\"{ps}\")\n",
    "            ps+=1\n",
    "            if status == 'train':\n",
    "                self.patches = self.patches + patches\n",
    "                for i in range(len(patches)):\n",
    "                    self.label.append(qs_std[idx])\n",
    "            else:\n",
    "                self.patches = self.patches + (torch.stack(patches),)  \n",
    "                self.label.append(qs_std[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patches)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.patches[idx], torch.Tensor([self.label[idx]]))\n",
    "\n",
    "if __name__=='__main__':\n",
    "    pass  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1008186c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import ToTensor,ToPILImage\n",
    "from torch.autograd import Variable as V \n",
    "from PIL import Image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97e56fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNDIQAnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNDIQAnet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 40, 5)\n",
    "        self.pool1 = nn.MaxPool2d(4)\n",
    "        self.conv2 = nn.Conv2d(40, 80, 5)\n",
    "        self.fc1   = nn.Linear(160, 1024)\n",
    "        self.fc2   = nn.Linear(1024, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x=x.view(-1,x.size(-3),x.size(-2),x.size(-1))\n",
    "        x  = self.conv1(x)\n",
    "        x  = self.pool1(x)\n",
    "        x  = self.conv2(x)\n",
    "        x1 = F.max_pool2d(x, (x.size(-2), x.size(-1)))\n",
    "        x2 = -F.max_pool2d(-x, (x.size(-2), x.size(-1)))\n",
    "        x  = torch.cat((x1, x2), 1)  \n",
    "        x  = x.squeeze(3).squeeze(2)\n",
    "        x  = F.relu(self.fc1(x))\n",
    "        x  = F.dropout(x)\n",
    "        x  = F.relu(self.fc2(x))\n",
    "        x  = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "if __name__=='__main__':\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "684da6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import os, yaml\n",
    "\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics.metric import Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ea3c98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DIQAPerformance(Metric):\n",
    "    def reset(self):\n",
    "        self.label_pred = []\n",
    "        self.label      = []\n",
    "\n",
    "    def update(self, output):\n",
    "        y_pred, y = output\n",
    "        self.label.append(y)\n",
    "        self.label_pred.append(torch.mean(y_pred))\n",
    "\n",
    "    def compute(self):\n",
    "        sq_std = np.reshape(np.asarray(self.label), (-1,))\n",
    "        sq_pred = np.reshape(np.asarray(self.label_pred), (-1,))\n",
    "        srocc = stats.spearmanr(sq_std, sq_pred)[0]\n",
    "        plcc = stats.pearsonr(sq_std, sq_pred)[0]\n",
    "        #plcc = stats.pearsonr(sq, q)[0]\n",
    "        return srocc, plcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90c84de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics.metric import Metric\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4c84b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_dir(path):\n",
    "    p=Path(path)\n",
    "    if not p.exists():\n",
    "        p.mkdir()\n",
    "\n",
    "def loss_fn(y_pred, y):\n",
    "    return F.l1_loss(y_pred, y) \n",
    "\n",
    "def get_data_loaders(dataset_name,config,train_batch_size):\n",
    "    datainfo=DataInfoLoader(dataset_name,config) \n",
    "    img_num=datainfo.img_num\n",
    "    index=np.arange(img_num)\n",
    "    np.random.shuffle(index)\n",
    "    \n",
    "    train_index=index[0:math.floor(img_num*0.6)]\n",
    "    val_index=index[math.floor(img_num*0.6):math.floor(img_num*0.8)]\n",
    "    test_index=index[math.floor(img_num*0.8):]\n",
    "    \n",
    "    \n",
    "    train_dataset = DIQADataset(dataset_name,config,train_index,status='train')\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                              batch_size=train_batch_size,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=4)\n",
    "\n",
    "    val_dataset = DIQADataset(dataset_name,config,val_index,status='val')\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset)\n",
    "\n",
    "    if config['test_ratio']:\n",
    "        test_dataset = DIQADataset(dataset_name,config,test_index,status='test')\n",
    "        test_loader = torch.utils.data.DataLoader(test_dataset)\n",
    "        return train_loader, val_loader, test_loader\n",
    "    return train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8be06e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solver:\n",
    "    def __init__(self):\n",
    "        self.model=CNNDIQAnet()\n",
    "\n",
    "    def run(self,dataset_name,train_batch_size,epochs,lr,weight_decay,model_name,config,trained_model_file,save_result_file,disable_gpu=False):\n",
    "        if config['test_ratio']:\n",
    "            train_loader, val_loader, test_loader = get_data_loaders(dataset_name,config,train_batch_size)\n",
    "        else:\n",
    "            train_loader, val_loader = get_data_loaders(dataset_name,config,train_batch_size)\n",
    "\n",
    "        device = torch.device(\"cuda\" if not disable_gpu and torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = self.model.to(device)\n",
    "        \n",
    "        optimizer = Adam(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        global best_criterion\n",
    "        best_criterion = -1 \n",
    "        trainer = create_supervised_trainer(self.model, optimizer, loss_fn, device=device)\n",
    "        evaluator = create_supervised_evaluator(self.model,metrics={'DIQA_performance': DIQAPerformance()},device=device)\n",
    "        print('Hello World')\n",
    "\n",
    "        @trainer.on(Events.EPOCH_COMPLETED)\n",
    "        def log_validation_results(engine):\n",
    "            evaluator.run(val_loader)\n",
    "            metrics = evaluator.state.metrics\n",
    "            SROCC, PLCC= metrics['DIQA_performance']\n",
    "            print(\"Validation Results - Epoch: {} SROCC: {:.4f} PLCC: {:.4f} \".format(engine.state.epoch, SROCC, PLCC))\n",
    "            global best_criterion\n",
    "            global best_epoch\n",
    "            if SROCC > best_criterion:\n",
    "                best_criterion = SROCC\n",
    "                best_epoch = engine.state.epoch\n",
    "                torch.save(self.model.state_dict(), trained_model_file)\n",
    "\n",
    "        @trainer.on(Events.EPOCH_COMPLETED)\n",
    "        def log_testing_results(engine):\n",
    "            if config[\"test_ratio\"] > 0 and config['test_during_training']:\n",
    "                evaluator.run(test_loader)\n",
    "                metrics = evaluator.state.metrics\n",
    "                SROCC,PLCC= metrics['DIQA_performance']\n",
    "                SROCC,PLCC= metrics['DIQA_performance']\n",
    "                print(\"Testing Results    - Epoch: {} SROCC: {:.4f} PLCC: {:.4f} \".format(engine.state.epoch, SROCC, PLCC))\n",
    "\n",
    "        @trainer.on(Events.COMPLETED)\n",
    "        def final_testing_results(engine):\n",
    "            if config[\"test_ratio\"] > 0:\n",
    "                self.model.load_state_dict(torch.load(trained_model_file,map_location ='cpu'))\n",
    "                evaluator.run(test_loader)\n",
    "                metrics = evaluator.state.metrics\n",
    "                SROCC,PLCC= metrics['DIQA_performance']\n",
    "                global best_epoch\n",
    "                print(\"Final Test Results - Epoch: {} SROCC: {:.4f} PLCC: {:.4f} \".format(best_epoch, SROCC, PLCC))\n",
    "                np.save(save_result_file, (SROCC, PLCC))\n",
    "        # kick everything off\n",
    "        trainer.run(train_loader, max_epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc03151",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-681c43bec640>:12: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  config=yaml.load(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp id: 1\n",
      "model: CNNDIQA\n",
      "{'test_during_training': True, 'train_ratio': 0.6, 'val_ratio': 0.2, 'test_ratio': 0.2, 'SOC': {'root': 'C:\\\\Users\\\\anujp\\\\Downloads\\\\databaserelease2\\\\databaserelease2\\\\jp2k', 'gt_file_path': './data/gt_files/LIVE_jp2k.xlsx', 'IQA_results_path': './results/'}}\n",
      "# Train Images: 136\n",
      "Index:\n",
      "[126 197 139 205 138 118 145   7  73 131   8 119  11 211 194 113  74  80\n",
      " 226 216 155 172 225 125 201 150 163  46 112 224 182 210 220 191  83 140\n",
      "  55 167  28 133 199 169 159  65  77 129 124  84 196 136 110 144 164 141\n",
      " 189  52  76 109 183  29 209  38  13 179 212 215  71  68  72 203  79  32\n",
      "  53  19  33  85  41  47 160  40  98  99  16 162 132  24  21 134 171  37\n",
      " 170  90  36 130  30  49 166  51 219 180  14 218  48 122 174  88  93  82\n",
      "  27 195   0  20 102  57  66 100 200 181 105 198 142  70  95 115 152 184\n",
      "   5 187  94  34   1 177  50 157  92 117]\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "# Val Images: 45\n",
      "Index:\n",
      "[ 39  96 111  18 188 213 186  89 176 207  10 217 135  69 127 114   6   9\n",
      " 161 101  22 121  12 116  63 128  43 193  61  59  42  67  44 221  86  91\n",
      " 146 143 190 192  75 173 168 149 154]\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "# Test Images: 46\n",
      "Index:\n",
      "[208  15 214 206 147 202  60 204   2 151  23 223  25  64 107 178  17   4\n",
      " 165 103  81  26  45   3 106 158 123  35 222  56 108 104  31 175 185 148\n",
      "  87  54  97  62 120 137  78 153 156  58]\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    dataset_name='SOC'\n",
    "    exp_id='1'\n",
    "    batch_size=128\n",
    "    epochs=2\n",
    "    lr=0.001\n",
    "    weight_decay=0.0\n",
    "    model='CNNDIQA'\n",
    "\n",
    "    with open('./config.yaml') as f:\n",
    "        config=yaml.load(f)\n",
    "    print('exp id: ' + exp_id)\n",
    "    print('model: ' + model)\n",
    "    print(config)\n",
    "    \n",
    "    ensure_dir('checkpoints')\n",
    "    trained_model_file = 'checkpoints/{}-{}-EXP{}-lr={}.pth'.format(model, dataset_name, exp_id, lr)\n",
    "    ensure_dir('results')\n",
    "    save_result_file = 'results/{}-{}-EXP{}-lr={}.npy'.format(model, dataset_name, exp_id, lr)\n",
    "\n",
    "  \n",
    "    \n",
    "    solver=Solver()\n",
    "    solver.run(dataset_name,batch_size, epochs, lr, weight_decay,model, \n",
    "               config,trained_model_file, save_result_file, disable_gpu='store_true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efe9acb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "from torch.autograd import Variable as V \n",
    "from scipy.signal import convolve2d\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4de32c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quality score is:0.8130279183387756\n"
     ]
    }
   ],
   "source": [
    "class CNNDIQAnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNDIQAnet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 40, 5)\n",
    "        self.pool1 = nn.MaxPool2d(4)\n",
    "        self.conv2 = nn.Conv2d(40, 80, 5)\n",
    "        self.fc1   = nn.Linear(160, 1024)\n",
    "        self.fc2   = nn.Linear(1024, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x=x.view(-1,x.size(-3),x.size(-2),x.size(-1))\n",
    "        x  = self.conv1(x)\n",
    "        x  = self.pool1(x)\n",
    "        x  = self.conv2(x)\n",
    "        x1 = F.max_pool2d(x, (x.size(-2), x.size(-1)))\n",
    "        x2 = -F.max_pool2d(-x, (x.size(-2), x.size(-1)))\n",
    "        x  = torch.cat((x1, x2), 1)  \n",
    "        x  = x.squeeze(3).squeeze(2)\n",
    "        x  = F.relu(self.fc1(x))\n",
    "        x  = F.dropout(x)\n",
    "        x  = F.relu(self.fc2(x))\n",
    "        x  = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def localNormalization(patch, P=3, Q=3, C=1):\n",
    "    kernel = np.ones((P,Q)) / (P * Q)\n",
    "    patch_mean = convolve2d(patch, kernel, boundary='symm', mode='same')\n",
    "    patch_sm = convolve2d(np.square(patch), kernel, boundary='symm', mode='same')\n",
    "    patch_std = np.sqrt(np.maximum(patch_sm - np.square(patch_mean), 0)) + C\n",
    "    patch_ln = (patch - patch_mean)/patch_std\n",
    "    return patch_ln\n",
    "\n",
    "def judgeAllOnesOrAllZreos(patch):\n",
    "    flag1=True\n",
    "    flag2=True\n",
    "    for i in range(patch.shape[0]):\n",
    "        for j in range(patch.shape[1]):\n",
    "            if patch[i,j]==255:\n",
    "                continue\n",
    "            else:\n",
    "                flag1=False\n",
    "\n",
    "    for i in range(patch.shape[0]):\n",
    "        for j in range(patch.shape[1]):\n",
    "            if patch[i,j]==0:\n",
    "                continue\n",
    "            else:\n",
    "                flag2=False\n",
    "    return flag1 or flag2\n",
    "\n",
    "def patchSifting(im, patch_size=48, stride=48):\n",
    "    img=np.array(im).copy()\n",
    "    im1=localNormalization(img)\n",
    "    im1=Image.fromarray(im1)\n",
    "    ret1,im2= cv2.threshold(img,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    w, h = im1.size\n",
    "    patches=()\n",
    "    for i in range(0, h - stride, stride):\n",
    "        for j in range(0, w - stride, stride):\n",
    "            patch = im2[i:i+patch_size,j:j+patch_size]\n",
    "            if judgeAllOnesOrAllZreos(patch)==False:\n",
    "                patch=to_tensor(im1.crop((j,i,j+patch_size,i+patch_size)))\n",
    "                patch=patch.float().unsqueeze(0)\n",
    "                patches = patches + (patch,)\n",
    "    return patches\n",
    "\n",
    "class Solver:\n",
    "    def __init__(self):\n",
    "        #pre-trained model path\n",
    "        self.model_path='./checkpoints/CNNDIQA-SOC-EXP916-lr=0.0001.pth'\n",
    "        #Initialize the model\n",
    "        self.model=CNNDIQAnet()\n",
    "\n",
    "    def quality_assessment(self,img_path):\n",
    "        #load the pre-trained model\n",
    "        self.model.load_state_dict(torch.load(self.model_path,map_location='cpu'))\n",
    "        im=Image.open(img_path).convert('L')\n",
    "        im=torch.stack(patchSifting(im))\n",
    "        #im=to_tensor(im).unsqueeze(0)\n",
    "        # if torch.cuda.is_available():\n",
    "        #     model=model.cuda()\n",
    "        #     im=im.cuda()\n",
    "        qs=self.model(im)\n",
    "        qs=qs.data.squeeze(0).cpu().numpy()[:,0].mean()\n",
    "        return qs\n",
    "if __name__=='__main__':\n",
    "    #Example\n",
    "    img_path='C:\\\\Users\\\\anujp\\\\Downloads\\\\databaserelease2\\\\databaserelease2\\\\jp2k\\\\img1.bmp'\n",
    "    solver=Solver()\n",
    "    qs=solver.quality_assessment(img_path)\n",
    "    print('quality score is:{}'.format(qs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0dbad1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy \n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6160c56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
